{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook serves as documentation for the reproducibility study of the library-chat-study in preparation for IMLS grant proposal.\n",
    "\n",
    "ALl of the Reproducibility Techniques have been implemented in \"TestScripts.ipynb.\"\n",
    "\n",
    "P1 Steps:\n",
    "1. Make your own copy of the library-chat-study either through Git or Collaboratory, etc.\n",
    "2. Run the P1 utility functions and comment out the importer for it or make sure you import correctly, as they are all definitions of the same functions.\n",
    "3. Download nltk wordnet via !pip install nltk if in collab, or using pip install nltk if not in collab.\n",
    "4. Insert the csv data needed, and copy its path for input into the function.\n",
    "5. Copy path for stop_words.txt for its input into the function.\n",
    "6. Run the function, put path for csv and txt\n",
    "7. Wait, as it outputs files.\n",
    "\n",
    "\n",
    "Gaps: \n",
    "1. Slight tweaking of the file was needed to make things work for step 1, which is standard.\n",
    "2. We can improve by perhaps adding different ways of preprocessing as per the literature review, as it's known that LDA can be tempered by different preprocessing.\n",
    "3. Since downstream issues occurred with respect to preprocessing, especially after changing the corpus structure, we can improve by adding more preprocessing techniques.\n",
    "\n",
    "\n",
    "P2 Steps:\n",
    "1. Make sure all libraries are loaded\n",
    "2. Run the p2 utility functions and comment out the importer for it or make sure you import correctly, as they are all definitions of the same functions.\n",
    "3. Run the function, put path for wholeChatsFile.txt into the step 1, because it used OS for path of files.\n",
    "4. Put number of topics (can be found using topic coherence, look at the curve and imput into step 2)\n",
    "5. Put number of words per topics (5-10 is a good number)\n",
    "6. Wait, as it's performing PyMallet LDA and might take a couple of minutes which is standard with these sorts of algorithms!\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "\n",
    "PyMallet LDA:\n",
    "\n",
    "\n",
    "Welcome to Phase 2 which runs the unsupervised topic modeling techniques. \n",
    "\n",
    "You should have first run Phase 1 to pre-process your chat data. \n",
    "It would generate cleaned chat files varying the parts of speech or question-only. \n",
    "Files generated are: wholeChatsFile.txt, wholeChatsFilePOS_N_ADJ_V.txt, \n",
    "wholeChatsFilePOS_N_ADJ.txt, and onlyQuestionsFile.txt.\n",
    "\n",
    "\n",
    "Step 1. Please input the pre-processed (.txt) file.\n",
    "(For example: \"wholeChatsFile.txt\"): /content/wholeChatsFile.txt\n",
    "\n",
    "Step 2. Please specify the number of topics. (suggested range 10-20)\n",
    " 10\n",
    "\n",
    "Step 3. Please specify the number of words per topics. (suggested range 5-10)\n",
    " 7\n",
    "\n",
    "\n",
    "Performing PyMallet LDA topic modeling -- please wait it might take a couple minutes!\n",
    "\n",
    "\n",
    "Results for PyMallet LDA  TC-PMI 0.000, TC-LCP 0.000, TC-NZ 0.000:\n",
    "\n",
    "\n",
    "rod article access specific answer shortly link\n",
    "\n",
    "\n",
    "book question rod find article how website\n",
    "\n",
    "\n",
    "will what find book rod when class\n",
    "\n",
    "\n",
    "rod uni article journal check today would\n",
    "\n",
    "\n",
    "how article work search doe librarian time\n",
    "\n",
    "\n",
    "email rod search check find contact database\n",
    "\n",
    "\n",
    "how rod find would print place collection\n",
    "\n",
    "\n",
    "check rod interlibrary copy number database renew\n",
    "\n",
    "\n",
    "book access how today room online will\n",
    "\n",
    "\n",
    "rod will article how search student link\n",
    "\n",
    "\n",
    "Performing LDA topic modeling -- please wait it might take a couple minutes!\n",
    "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
    "  warnings.warn(msg, category=FutureWarning)\n",
    "\n",
    "\n",
    "Results for LDA (with sklearn)  TC-PMI 0.000, TC-LCP 0.000, TC-NZ 0.000:\n",
    "\n",
    "\n",
    "rod article check search book wa link\n",
    "\n",
    "\n",
    "rod article search book check link access\n",
    "\n",
    "\n",
    "rod article book link wa search research\n",
    "\n",
    "\n",
    "rod article book search check link wa\n",
    "\n",
    "\n",
    "rod article search link work moment book\n",
    "\n",
    "\n",
    "rod article search book check link wa\n",
    "\n",
    "\n",
    "rod article link check search book day\n",
    "\n",
    "\n",
    "rod article book search check link wa\n",
    "\n",
    "\n",
    "rod book article link search check wa\n",
    "\n",
    "\n",
    "rod search book link article access wa\n",
    "\n",
    "\n",
    "(Not meaningful results with topics that are not related to the chat data)\n",
    "\n",
    "\n",
    "Errors:\n",
    "\n",
    "\n",
    "Original code:  tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=max_features, stop_words='english')                                        \n",
    "\n",
    "\n",
    "Fix:            tf_vectorizer = CountVectorizer(max_features=max_features, stop_words='english') - going back to default values for max and min because raised value error\n",
    "\n",
    "\n",
    "Result:         Did not work because the max_df and min_df need to be tweaked in such a way that there is a NaN or inf in the matrix being inverted within the code.\n",
    "\n",
    "\n",
    "\n",
    "Error: TF-IDF & LSA did not work due to pre-processing issues, which is why we are using PyMallet LDA.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Gaps:\n",
    "1. Since we are using PyMallet, perhaps we can explore other implementations of LDA if needed.\n",
    "2. Perhaps we should explore methods that take less time (if that is an issue).\n",
    "3. Because pLSA was found to be one of the most efficient methods of topic modeling in the original paper, we might try to use Pachinko Allocation Model (PAM) in MALLET, another technique which improves LDA on greater expressibility.\n",
    "\n",
    "\n",
    "P3 Steps:\n",
    "1. Make sure all libraries are loaded\n",
    "2. Make sure you previously ran the P1 and P2 steps, then proceed to run P3 utility functions.\n",
    "3. Make sure you fix the issue with GuidedLDA, using the correct GitHub link found at https://github.com/dex314/GuidedLDA_WorkAround.\n",
    "4. Make sure anchors.txt is created, as that will be necessary for running the function.\n",
    "5. Run the function, inputting the wholeChatsFile.txt path, the anchors.txt path, the number of topics, and the number of words per topic.\n",
    "\n",
    "\n",
    "Errors:\n",
    "\n",
    "\n",
    "We find that there was an all zero column in the document-term matrix for GuidedLDA.\n",
    "\n",
    "\n",
    "This might have necessitated from issues with preprocessing, or with the fact that GuidedLDA is now prone to issues on Windows OS, including Google Collaboratory.\n",
    "\n",
    "\n",
    "Gaps:\n",
    "1. The GuidedLDA website notes that if we are working with a larger corpus, we may want to use more sophisticated methods of topic modeling such as those provided in hca and MALLET.\n",
    "2. Preprocessing needs to be updated and refined to better handle the issues with downstream issues, such as errors with LDA.\n",
    "3. There are hyperparameters that can be tuned in the LDA models, and possible other models that utilize different parameters. For example, the number of topics can be tuned as well as the number of words per topic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43794f9dbf35c8071020a62fff6bd77ce265685b8e1bb5f42a73b85f9efdf7ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
