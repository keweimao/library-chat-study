{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eDK9HRYgX98",
        "outputId": "25a5b93e-a52e-4e03-b31e-be8672fb00a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\jrubi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\jrubi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jrubi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in c:\\users\\jrubi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jrubi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.8.17)\n",
            "Requirement already satisfied: colorama in c:\\users\\jrubi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iuvlwoWfW2-",
        "outputId": "65baa784-0ec6-4af0-e911-584c40de76e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\jrubi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\jrubi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\jrubi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz0IXAr4NtKS",
        "outputId": "b2f793ea-39ed-4d48-e12b-cd191f0db953"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk.download('stopwords')\n",
        "# Path in Collab: /root/nltk_data/corpora/stopwords/english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vq5xIv5ge_hg"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-b33b73cc8edb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
          ]
        }
      ],
      "source": [
        "#P1 Utility\n",
        "\n",
        "\"\"\" File:  P1_utility_functions.py\n",
        "    Utility functions used in Phase 1 Pre-processing of the raw chat (.csv) file\n",
        "\"\"\"\n",
        "import os.path\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def getStopWords(stopWordFileName):\n",
        "    \"\"\"Reads stop-words text file which is assumed to have one word per line.\n",
        "       Returns stopWordDict.\n",
        "    \"\"\"\n",
        "    stopWordDict = {}\n",
        "    stopWordFile = open(stopWordFileName, 'r')\n",
        "\n",
        "    for line in stopWordFile:\n",
        "        word = line.strip().lower()\n",
        "        stopWordDict[word] = None\n",
        "        \n",
        "    return stopWordDict\n",
        "\n",
        "def getFileName(prompt):\n",
        "    \"\"\"Prompts the user for a valid file which it returns.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        fileName = input(prompt+\" \")\n",
        "        if os.path.exists(fileName):\n",
        "            return fileName\n",
        "        else:\n",
        "            print(\"File not found! Make sure that the file is inside this directory.\")\n",
        "\n",
        "def readRawChats(inFile):\n",
        "    \"\"\"\n",
        "        Reads .csv file and split into transcripts by splitting on the Timestamp which includes the Date.\n",
        "        The returned transcriptList is a list-of-lists where each \"outer\" list item contains information about\n",
        "        a single chat.  \n",
        "    \"\"\"\n",
        "\n",
        "    inFile = open(inFile, \"r\")  # NOTE .csv file assumed to have column-headings line\n",
        "\n",
        "    dateAtStartCount = 0\n",
        "    transcriptList = []\n",
        "    currentTranscriptLines = []\n",
        "\n",
        "    for line in inFile:\n",
        "        frontOfLine = line[:6]\n",
        "        if frontOfLine.count(\"/\") == 2:\n",
        "            dateAtStartCount += 1\n",
        "            if dateAtStartCount == 1: #ignore header line\n",
        "                currentTranscriptLines = [line.strip()]\n",
        "            else:\n",
        "                transcriptList.append(currentTranscriptLines)\n",
        "                currentTranscriptLines = [line.strip()]\n",
        "        else:\n",
        "            currentTranscriptLines.append(line.strip())\n",
        "    transcriptList.append(currentTranscriptLines)\n",
        "    \n",
        "    return transcriptList\n",
        "\n",
        "\n",
        "def findInitialQuestion(transList, transIndex):\n",
        "    \"\"\"\n",
        "        Takes in transList which is a list of strings containing the information about a single chat.\n",
        "        The index 0 string will contain the Initial Question field, which it returns if it exists; otherwise\n",
        "        None is returned.\"\n",
        "    \"\"\"\n",
        "    \n",
        "    firstCommaIndex = transList[0].find(\",\")\n",
        "    if firstCommaIndex == -1:\n",
        "        print(\"First comma not found\")\n",
        "        return None\n",
        "    else:\n",
        "        secondCommaIndex = transList[0].find(\",\",firstCommaIndex+1)\n",
        "        if secondCommaIndex == -1:\n",
        "            print(\"Second comma not found\")\n",
        "            return None\n",
        "        else:\n",
        "            thirdCommaIndex = transList[0].find(\",\",secondCommaIndex+1)\n",
        "            if thirdCommaIndex == -1:\n",
        "                thirdCommaIndex = len(transList[0])-1\n",
        "           \n",
        "            #print(secondCommaIndex, thirdCommaIndex)\n",
        "            if secondCommaIndex + 1 == thirdCommaIndex:\n",
        "                return None\n",
        "            else:\n",
        "                return transList[0][secondCommaIndex+1:thirdCommaIndex]\n",
        "\n",
        "            \n",
        "def generateTranscriptDialogList(trans):\n",
        "    \n",
        "    transcriptDialogList = []\n",
        "    transStr = \" \".join(trans)  # merge transcript back to a single string\n",
        "\n",
        "    #split by time-stamps to get a dialogList\n",
        "    transTimeIndexList = []\n",
        "    for index in range(2,len(transStr)-6):\n",
        "        if transStr[index] == \":\" and transStr[index+3] == \":\" and transStr[index+1:index+3].isdigit() and transStr[index+4:index+6].isdigit():\n",
        "            transTimeIndexList.append(index-2)\n",
        "    dialogList = []\n",
        "    for i in range(len(transTimeIndexList)-1):\n",
        "        dialogList.append(transStr[transTimeIndexList[i]:transTimeIndexList[i+1]])\n",
        "    if len(transTimeIndexList) == 0:\n",
        "        dialogList.append(transStr)\n",
        "    else:\n",
        "        dialogList.append(transStr[transTimeIndexList[-1]:])\n",
        "    \n",
        "    return dialogList    \n",
        "\n",
        "def findInitialQuestionInDialog(dialogList, chatIndex):\n",
        "    \"\"\" If the 'Initial question' column in the .csv file was empty, this function is called\n",
        "        to find and return the initial question from the chat dialog.\"\"\"\n",
        "\n",
        "    for i in range(len(dialogList)):\n",
        "        helpYouCount = dialogList[i].lower().count(\"help you\")\n",
        "        welcomeCount = dialogList[i].lower().count(\"welcome\")\n",
        "        infoDeskCount = dialogList[i].lower().count(\"info desk\")\n",
        "        try:\n",
        "            if helpYouCount == 0 and welcomeCount == 0 and infoDeskCount == 0 and len(dialogList[i]) >= 40:\n",
        "                return dialogList[i]\n",
        "                \n",
        "        except:\n",
        "            print(\"\\n\\nNO QUESTION FOUND! \",chatIndex)\n",
        "            break\n",
        "\n",
        "def removeTags(fileStr):\n",
        "    \"\"\"\n",
        "        Removes all tags from the chat that start with '<xyz' and end with '</xyz'.\n",
        "    \"\"\"\n",
        "    current = 0\n",
        "    while True:\n",
        "        #print(\"Next char:\",fileStr[current])\n",
        "        openAngleBracketIndex = fileStr.find('<',current)\n",
        "        if openAngleBracketIndex == -1:\n",
        "            break\n",
        "        spaceIndex = fileStr.find(' ', openAngleBracketIndex+1)\n",
        "        if spaceIndex == -1:\n",
        "            break\n",
        "        else:\n",
        "            current = spaceIndex\n",
        "        endStr = \"</\"+fileStr[openAngleBracketIndex+1:spaceIndex]+'>'\n",
        "\n",
        "        endIndex = fileStr.find(endStr, spaceIndex)\n",
        "        if endIndex == -1:\n",
        "            current = spaceIndex\n",
        "        else:\n",
        "            endIndex = endIndex+len(endStr)\n",
        "\n",
        "            #print(openAngleBracketIndex, endStr, endIndex+len(endStr))\n",
        "            fileStr = fileStr[:openAngleBracketIndex]+ \\\n",
        "                      fileStr[endIndex:]\n",
        "            #print(fileStr)\n",
        "            current = openAngleBracketIndex\n",
        "    return fileStr\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "NOTE: The nltk.pos_tag function returns the Penn Treebank tag for the word but we just want\n",
        "whether the word is a noun, verb, adjective or adverb. We need a short simplification routine to translate from\n",
        "the Penn tag to a simpler tag.\n",
        "\"\"\"\n",
        "def simplify(penn_tag):\n",
        "    \"\"\" Simplify Penn tags to n (NOUN), v (VERB), a (ADJECTIVE) or r (ADVERB)\"\"\"\n",
        "    pre = penn_tag[0]\n",
        "    \n",
        "    if pre == 'J':\n",
        "        return 'a'\n",
        "    elif pre == 'R':\n",
        "        return 'r'\n",
        "    elif pre == 'V':\n",
        "        return 'v'\n",
        "    elif pre == 'N':\n",
        "        return 'n'\n",
        "    else:\n",
        "        return 'r'\n",
        "        return 'other'\n",
        "\n",
        "def preprocess(text, stop_words, POS_list):\n",
        "    \"\"\" Preprocesses the text to remove stopwords, lemmatizes each word and only includes\n",
        "        words that are POS in the global POS_LIST\"\"\"\n",
        "\n",
        "    toks = gensim.utils.simple_preprocess(str(text), deacc=True)\n",
        "    wn = WordNetLemmatizer()\n",
        "    return [wn.lemmatize(tok, simplify(pos)) for tok, pos in nltk.pos_tag(toks)\n",
        "            if tok not in stop_words and simplify(pos) in POS_list]\n",
        "        \n",
        "def writeInitialQuestion(chatIndexInCSV, questionFile,  wholeChatsFileTxt, question, questionCount, stopWordsDict, POS_list):\n",
        "    \"\"\" Write a cleaned up version of the initial question to the question file. \"\"\"\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    cleanQuestion = \"\"\n",
        "    question = question.lower()\n",
        "\n",
        "    colonCount = question.count(\":\")\n",
        "\n",
        "    if colonCount >= 3:  # time-stamp ##:##:## - person: question\n",
        "        colonOneIndex = question.find(\":\")\n",
        "        colonTwoIndex = question.find(\":\", colonOneIndex+1)\n",
        "        colonThreeIndex = question.find(\":\", colonTwoIndex+1)\n",
        "        question = question[colonThreeIndex+1:]\n",
        "    elif colonCount >= 1:\n",
        "        colonOneIndex = question.find(\":\")\n",
        "        question = question[colonOneIndex+1:]\n",
        "        \n",
        "    question = question.replace('&#x27;', \"'\")\n",
        "    question = question.replace('&#x2F;', \" \")\n",
        "    question = question.replace('&nbsp;', \" \")\n",
        "    question = question.replace('&quot;','\"')\n",
        "\n",
        "    ### HERE CLEAN UP <xyz ......</xyz>, e.g., <a href.....</a>, <span ... </span>\n",
        "\n",
        "    question = removeTags(question)\n",
        "    question = question.replace('.','Z')\n",
        "    question = question.replace('!','Z')\n",
        "    question = question.replace('?','Z')\n",
        "    \n",
        "    masterWordList = []\n",
        "    sentenceList = question.split(\"Z\")\n",
        "    for question in sentenceList:\n",
        "        wordList = question.split()\n",
        "        cleanQuestion = \"\"\n",
        "        for word in wordList:\n",
        "            cleanWord = \"\"\n",
        "            for char in word:\n",
        "                if char >= 'a' and char <= 'z':\n",
        "                    cleanWord += char\n",
        "            if len(cleanWord) > 0 and len(cleanWord) < 30:  #upper bound to eliminate url's\n",
        "                cleanQuestion += lemmatizer.lemmatize(cleanWord) + \" \"\n",
        "        pos_wordList = preprocess(cleanQuestion, stopWordsDict, POS_list)\n",
        "          \n",
        "        masterWordList.extend(pos_wordList)\n",
        "\n",
        "    chatCleaned = \" \".join(masterWordList)\n",
        "    if len(chatCleaned) > 0:\n",
        "        questionFile.write(chatCleaned)\n",
        "        wholeChatsFileTxt.write(chatCleaned)\n",
        "        questionCount += 1\n",
        "    return questionCount\n",
        "\n",
        "def writeChatDialog(excelLineNumber, wholeChatsFile,  wholeChatsFileTxt, dialogList, stopWordsDict, POS_list):\n",
        "    \"\"\" Writes a chat's dialog to a line in the text file. \"\"\"\n",
        "    for i in range(len(dialogList)):\n",
        "      \n",
        "        writeInitialQuestion(excelLineNumber, wholeChatsFile,  wholeChatsFileTxt, dialogList[i], 0, stopWordsDict, POS_list)\n",
        "        wholeChatsFile.write(\" \")  # separate end of this line with start of next line\n",
        "        wholeChatsFileTxt.write(\" \")  # separate end of this line with start of next line\n",
        "        \n",
        "   \n",
        "def writeWholeChatsToFile(transcriptDialogList, dataFileName, stopWordsDict, POS_list):\n",
        "    \"\"\" Writes a whole chat's dialog one per line to a text file.  Removed from\n",
        "        the line of text is:\n",
        "        1) time-stamps and names:  e.g., '13:45:42 - Jordan:'\n",
        "        2) all punctuations\n",
        "    \"\"\"\n",
        "\n",
        "    wholeChatsFile = open(dataFileName+\".csv\", \"w\")\n",
        "    wholeChatsFileTxt = open(dataFileName+\".txt\", \"w\")\n",
        "    wholeChatsCount = 0\n",
        "    for transcriptDialog in transcriptDialogList:\n",
        "\n",
        "        if transcriptDialog[1] is not None:\n",
        "            wholeChatsFile.write(str(transcriptDialog[0])+\",\")\n",
        "\n",
        "            # check to see if initial question is already in the chat dialog\n",
        "            timeStampAndNameList = re.findall(r'[0-9][0-9]:[0-9][0-9]:[0-9][0-9] - [\\w\\s]+:', transcriptDialog[1])\n",
        "            \n",
        "            if len(timeStampAndNameList) == 0:  # no time-stamp so from 'initial question' column of .csv\n",
        "                # write initial question to file since it is not part of the chat dialog\n",
        "                writeInitialQuestion(transcriptDialog[0], wholeChatsFile, wholeChatsFileTxt, transcriptDialog[1], 0, stopWordsDict, POS_list)\n",
        "                wholeChatsFile.write(\" \")\n",
        "                wholeChatsFileTxt.write(\" \")\n",
        "            writeChatDialog(transcriptDialog[0],wholeChatsFile,  wholeChatsFileTxt, transcriptDialog[2], stopWordsDict, POS_list)\n",
        "            \n",
        "            #wholeChatsFile.write(\"\\n\")\n",
        "            wholeChatsCount += 1\n",
        "            wholeChatsFile.write(\"\\n\")\n",
        "            wholeChatsFileTxt.write(\"\\n\")\n",
        "    print(\"Whole Chats Count:\", wholeChatsCount, \"written to\",dataFileName+\".txt\")\n",
        "    wholeChatsFile.close()\n",
        "    wholeChatsFileTxt.close()\n",
        "\n",
        "def writeQuestionsOnlyToFile(transcriptDialogList, dataFileName, stopWordsDict, POS_list):\n",
        "    \"\"\" Writes only the initial questions one per line to a text file. \n",
        "    \"\"\"\n",
        "    questionFile = open(dataFileName+\".csv\", \"w\")\n",
        "    questionTxtFile = open(dataFileName+\".txt\", \"w\")\n",
        "    questionCount = 0\n",
        "    for transcriptDialog in transcriptDialogList:\n",
        "        if transcriptDialog[1] is not None:\n",
        "            currentCount = questionCount\n",
        "            questionCount = writeInitialQuestion(transcriptDialog[0], questionFile, questionTxtFile, transcriptDialog[1], questionCount, stopWordsDict, POS_list)\n",
        "            if currentCount < questionCount:\n",
        "                questionFile.write(\"\\n\")\n",
        "                questionTxtFile.write(\"\\n\")\n",
        "    print(\"Total Question Count:\", questionCount, \"written to\",dataFileName+\".txt\")\n",
        "    questionFile.close()\n",
        "    questionTxtFile.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2PB-vq6esTJ",
        "outputId": "8b65fabc-6c82-4679-c34d-b44aeb81c9e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Phase 1 of the chat analysis which pre-processes a raw chat data .csv file \n",
            "from the LibChat keeping only the 5 columns (with column-headings): \n",
            "Timestamp, Duration (seconds), Initial Question, Message Count, and Transcript. \n",
            "\n",
            "Running Phase 1 to pre-process your raw chat data (.csv) will generate four cleaned chat \n",
            "files varying the parts of speech or question-only. \n",
            "1) \"onlyQuestionsFile.txt\" - consists of only the initial questions asked by the library patrons \n",
            "2) \"wholeChatsFile.txt\" - consists of the whole cleaned chat transcripts \n",
            "3) \"wholeChatsFilePOS_N_ADJ.txt\" - consists of only the nouns and adjectives parts-of-speech (POS) \n",
            "4) \"wholeChatsFilePOS_N_ADJ_V.txt\" - consists of only the nouns, adjectives, and verbs parts-of-speech\n",
            "\n",
            "\n",
            "Step 1. Please input the raw LibChat (.csv) file.\n",
            "(For example: \"chatFile.csv\"): /content/LibChat.csv\n",
            "\n",
            "Step 2. Please input the stop words (.txt) file.\n",
            "(For example: \"stop_words.txt\"): /root/nltk_data/corpora/stopwords/english\n",
            "\n",
            "\n",
            "WARNING:  Depending on the size of your chat data file.  This step might take several minutes.\n",
            "Number of initial questions from Initial Question column of .csv: 1\n",
            "Total Question Count: 1 written to onlyQuestionsFile.txt\n",
            "Whole Chats Count: 1 written to wholeChatsFile.txt\n",
            "Whole Chats Count: 1 written to wholeChatsFilePOS_N_ADJ.txt\n",
            "Whole Chats Count: 1 written to wholeChatsFilePOS_N_ADJ_V.txt\n"
          ]
        }
      ],
      "source": [
        "#P1 Script\n",
        "\n",
        "\"\"\" File:  P1_preprocess_data.py\n",
        "    Description:  Takes as input raw chat data .csv file from the LibChat keeping only the columns:\n",
        "    Timestamp, Duration (seconds), Initial Question, Message Count, and Transcript\n",
        "\n",
        "    Additionally the chat text data is \"cleaned\" by: \n",
        "    1) removing timestamps, \n",
        "    2) removing chat patron and librarian identifiers, \n",
        "    3) removing http tags (e.g., URLs), \n",
        "    4) removing non-ASCII characters,\n",
        "    5) removing stopwords, and \n",
        "    6) lemmatized words using nltk.WordNetLemmatizer() \n",
        "\n",
        "    Four data-set versions of the “cleaned” chat transcripts were prepared:\n",
        "    1) \"onlyQuestionsFile.txt\" - Questions only: consists of only the initial question asked by\n",
        "        the library patron in each chat transcript\n",
        "    2) \"wholeChatsFile.txt\" - Whole chats: consists of the whole cleaned chat transcripts\n",
        "    3) \"wholeChatsFilePOS_N_ADJ.txt\" - Whole chats with POS (Noun and Adjective): consists of only\n",
        "       the nouns and adjectives parts-of-speech (POS) from the whole cleaned chat transcripts\n",
        "    4) \"wholeChatsFilePOS_N_ADJ_V.txt\" - Whole chats with POS (Noun, Adjective, and Verb): consists\n",
        "       of only the nouns, adjectives, and verbs parts-of-speech (POS) from the whole cleaned chat transcripts\n",
        "    The goal of the first two data sets was to see if looking at only the initial question in the\n",
        "    chats was better than the whole chats. The goal of the last two data sets was to see if varying\n",
        "    the parts-of-speech retained had any effect on the topic modeling analyses. \n",
        "\n",
        "    Takes as input raw chat data .csv file and produces a list-of-lists called transcriptDialogList with a format:\n",
        "    [[<excel index int>, \"Initial question string\", [Transcript split by chat responses which including initial\n",
        "    question]], ...]. This transcriptDialogList is used to write two text files for each of the four\n",
        "    data-set versions .  Each chat dialog is used to produce one line in the two text files:\n",
        "    1) the .csv file is formated with one chat per line formatted as:\n",
        "       chat line # in original .csv, cleaned and pre-processed text of the chat, and\n",
        "    2) the .txt file is cleaned and pre-processed text of the chat\n",
        "\n",
        "\"\"\"\n",
        "import nltk\n",
        "\n",
        "#from P1_utility_functions import *\n",
        "\n",
        "def main():\n",
        "    print('Welcome to Phase 1 of the chat analysis which pre-processes a raw chat data .csv file',\n",
        "          '\\nfrom the LibChat keeping only the 5 columns (with column-headings):',\n",
        "          '\\nTimestamp, Duration (seconds), Initial Question, Message Count, and Transcript.',\n",
        "          '\\n\\nRunning Phase 1 to pre-process your raw chat data (.csv) will generate four cleaned chat',\n",
        "          '\\nfiles varying the parts of speech or question-only.',\n",
        "          '\\n1) \"onlyQuestionsFile.txt\" - consists of only the initial questions asked by the library patrons',\n",
        "          '\\n2) \"wholeChatsFile.txt\" - consists of the whole cleaned chat transcripts',\n",
        "          '\\n3) \"wholeChatsFilePOS_N_ADJ.txt\" - consists of only the nouns and adjectives parts-of-speech (POS)',\n",
        "          '\\n4) \"wholeChatsFilePOS_N_ADJ_V.txt\" - consists of only the nouns, adjectives, and verbs parts-of-speech\\n')\n",
        "\n",
        "    prompt = \"\\nStep 1. Please input the raw LibChat (.csv) file.\" + \\\n",
        "             '\\n(For example: \"chatFile.csv\"):'\n",
        "    inputCSVFileName = getFileName(prompt)\n",
        "\n",
        "    prompt = \"\\nStep 2. Please input the stop words (.txt) file.\" + \\\n",
        "             '\\n(For example: \"stop_words.txt\"):'\n",
        "    stopWordFileName = getFileName(prompt)\n",
        "\n",
        "    print(\"\\n\\nWARNING:  Depending on the size of your chat data file.  This step might take several minutes.\")\n",
        "\n",
        "    POS_list = ['n','a','v','r']  # n - noun and a - adjective other possibilities: v -verb, r - adverb, 'other'\n",
        "\n",
        "    stopWordsDict = getStopWords(stopWordFileName)\n",
        "    transcriptList = readRawChats(inputCSVFileName)\n",
        "\n",
        "    initialQuestionCount = 0\n",
        "    transIndex = 2  # Assumes Excel .cvs had a column-header in line 1\n",
        "    transcriptDialogList = []\n",
        "    for trans in transcriptList:\n",
        "        transDialogList = generateTranscriptDialogList(trans)\n",
        "        initialQuestion = findInitialQuestion(trans, transIndex)\n",
        "        if initialQuestion == None:\n",
        "            initialQuestion = findInitialQuestionInDialog(transDialogList,transIndex)           \n",
        "        else:\n",
        "            initialQuestionCount+= 1\n",
        "            \n",
        "        transcriptDialogList.append([transIndex, initialQuestion, transDialogList])\n",
        "        transIndex += 1\n",
        "\n",
        "    print(\"Number of initial questions from Initial Question column of .csv:\", initialQuestionCount)\n",
        "\n",
        "    POS_list = ['n','a','v','r','other']  # n - noun and a - adjective other possibilities: v -verb, r - adverb, 'other'\n",
        "    writeQuestionsOnlyToFile(transcriptDialogList, \"onlyQuestionsFile\", stopWordsDict, POS_list)\n",
        "\n",
        "    writeWholeChatsToFile(transcriptDialogList, \"wholeChatsFile\", stopWordsDict, POS_list)\n",
        "\n",
        "    POS_list = ['n','a']  # n - noun and a - adjective other possibilities: v -verb, r - adverb, 'other'\n",
        "    writeWholeChatsToFile(transcriptDialogList, \"wholeChatsFilePOS_N_ADJ\", stopWordsDict, POS_list)\n",
        "\n",
        "    POS_list = ['n','a','v']  # n - noun and a - adjective other possibilities: v -verb, r - adverb, 'other'\n",
        "    writeWholeChatsToFile(transcriptDialogList, \"wholeChatsFilePOS_N_ADJ_V\", stopWordsDict, POS_list)\n",
        "    \n",
        "    return transcriptDialogList\n",
        "    \n",
        "t = main()  # start main running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i_YPBp6Th9NH"
      },
      "outputs": [],
      "source": [
        "\"\"\" File:  P2_utility_functions.py\n",
        "    Unility functions for Phase 2 which performs the topic modeling.\n",
        "\n",
        "    Latent Dirichlet Allocation (LDA), PyMallet\n",
        "    Here we are used the LDA implementation from GitHub PyMallet at:\n",
        "    https://github.com/mimno/PyMallet\n",
        "    The LDA code below is based on their lda_reference.py code written in Python\n",
        "    The PyMallet project has an MIT License see below.\n",
        "================================================================================\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 mimno\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "===========================================================================\n",
        "\"\"\"\n",
        "from pprint import pprint  # pretty-printer\n",
        "from collections import defaultdict\n",
        "from gensim import corpora\n",
        "from six import iteritems\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from time import time\n",
        "\n",
        "\n",
        "def getStopWords(stopWordFileName):\n",
        "    \"\"\"Reads stop-words text file which is assumed to have one word per line.\n",
        "       Returns stopWordDict.\n",
        "    \"\"\"\n",
        "    stopWordDict = {}\n",
        "    stopWordFile = open(stopWordFileName, 'r')\n",
        "\n",
        "    for line in stopWordFile:\n",
        "        word = line.strip().lower()\n",
        "        stopWordDict[word] = None\n",
        "    stopWordSet = set(stopWordDict)\n",
        "        \n",
        "    return stopWordDict, stopWordSet\n",
        "\n",
        "def getPositiveInteger(prompt):\n",
        "    \"\"\"Prompts the user for a valid positive integer which it returns.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        inputStr = input(prompt+\" \")\n",
        "        try:\n",
        "            intValue = int(inputStr)\n",
        "            if intValue <= 0:\n",
        "                print(\"Please enter a positive integer.\")\n",
        "                raise ValueError(\"positive integer only\")\n",
        "            return intValue\n",
        "        except:\n",
        "            print(\"Invalid positive integer\")\n",
        "\n",
        "def getFileName(prompt):\n",
        "    \"\"\"Prompts the user for a valid file which it returns.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        fileName = input(prompt+\" \")\n",
        "        if os.path.exists(fileName):\n",
        "            return fileName\n",
        "        else:\n",
        "            print(\"File not found! Make sure that the file is inside this directory.\")\n",
        "\n",
        "# used in LDA sklearn\n",
        "def readChatCorpusFile(chatFileName):\n",
        "    \"\"\" Read specified chat corpus file (which should be a preprocessed\n",
        "        text file with one chat per line) and returns the documents list\n",
        "        where each chat being a string in the list.\n",
        "    \"\"\"\n",
        "    documentsFile = open(chatFileName, 'r')\n",
        "\n",
        "    documentsList = []\n",
        "    for documentLine in documentsFile:\n",
        "        documentLine = documentLine.lower()\n",
        "        if len(documentLine) > 0:\n",
        "            documentsList.append(documentLine)\n",
        "    #print(\"len(documents)\",len(documentsList))\n",
        "    return documentsList\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    \"\"\" Displays the specified top topics and top words to screen\"\"\"\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        \n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        message += \"\\n \"\n",
        "        message += \" \".join([feature_names[i]+\" (\"+str(model.components_[topic_idx][i])+\")\\n\"\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "def write_file_top_words(model, feature_names, n_top_words, fileName):\n",
        "    \"\"\" Writes the specified top topics and top words to the specified fileName\"\"\"\n",
        "    outputFile = open(\"raw_\"+fileName, 'w')\n",
        "    outputFileTopics = open(fileName, 'w')\n",
        "    outputFile.write(\"File: \"+\"raw_\"+fileName+\"\\n\\n\")\n",
        "    outputFileTopics.write(\"File: \"+fileName+\"\\n\\n\")\n",
        "    topicList = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        topicStr = \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        topicList.append(topicStr)\n",
        "        outputFileTopics.write(topicStr+\"\\n\")\n",
        "\n",
        "        message += topicStr + \"\\n \"\n",
        "        message += \" \".join([feature_names[i]+\" (\"+str(model.components_[topic_idx][i])+\")\\n\"\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        outputFile.write(message+\"\\n\")\n",
        "\n",
        "    outputFile.close()\n",
        "    outputFileTopics.close()\n",
        "    return topicList\n",
        "\n",
        "# used in LSA with gensim\n",
        "def write_LSA(model_lsi, n_topics, n_words_per_topic, fileName):\n",
        "    \"\"\" Writes the specified top topics and top words to the specified fileName\"\"\"\n",
        "    topicList = model_lsi.print_topics(n_topics)\n",
        "    ##print(\"topicList\",topicList)\n",
        "    corpus_tfidf_and_lsa_fileName = \"raw_\" + fileName\n",
        "    corpus_tfidf_and_lsa_file = open(corpus_tfidf_and_lsa_fileName, 'w')\n",
        "    outputFile = open(fileName,'w')\n",
        "    corpus_tfidf_and_lsa_file.write(\"File: \" + corpus_tfidf_and_lsa_fileName + '\\n\\n')\n",
        "    outputFile.write(\"File: \" + fileName + '\\n\\n')\n",
        "    listOfTopics = []\n",
        "    for topicIndex in range(n_topics):\n",
        "        corpus_tfidf_and_lsa_file.write(str(topicList[topicIndex])+'\\n')\n",
        "        line = str(topicList[topicIndex])\n",
        "        topicString = \"\"\n",
        "        startIndex = 0\n",
        "        for count in range(n_words_per_topic):\n",
        "            wordStart = line.find('*\"', startIndex) + 2\n",
        "            wordEnd = line.find('\"', wordStart) - 1\n",
        "            topicString += line[wordStart:wordEnd+1] + \" \"\n",
        "            startIndex = wordEnd + 1\n",
        "        outputFile.write(topicString+\"\\n\")\n",
        "        listOfTopics.append(topicString)\n",
        "\n",
        "    outputFile.close()\n",
        "    corpus_tfidf_and_lsa_file.close()\n",
        "    return listOfTopics\n",
        "\n",
        "\n",
        "# create a vector stream to avoid loading the whole vector into memory at one time\n",
        "class MyCorpus(object):\n",
        "    def __init__(self, documentsList, dictionary):\n",
        "        self._docsList = documentsList\n",
        "        self.myDictionary = dictionary\n",
        "        \n",
        "    def __iter__(self):\n",
        "#        for line in open(FILE_NAME_OF_CORPUS+'_lemmatized.txt'):\n",
        "        for line in self._docsList:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield self.myDictionary.doc2bow(line.lower().split())\n",
        "\n",
        "\n",
        "def createCorpusDictionary(documentsList, stoplist):\n",
        "    # collect statistics about all tokens, i.e., words\n",
        "    dictionary = corpora.Dictionary(line.lower().split() for line in documentsList)\n",
        "    # remove stop words and words that appear only once\n",
        "##    stop_ids = [\n",
        "##        dictionary.token2id[stopword]\n",
        "##        for stopword in stoplist\n",
        "##        if stopword in dictionary.token2id\n",
        "##    ]\n",
        "    once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
        "##    dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
        "    dictionary.filter_tokens(once_ids)  # remove words that appear only once\n",
        "    dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
        "\n",
        "    #dictionary.save(FILE_NAME_OF_CORPUS+'.dict')  # store the dictionary, for future reference\n",
        "    corpus_memory_friendly = MyCorpus(documentsList,dictionary)  # doesn't load the corpus into memory!\n",
        "\n",
        "    #corpora.MmCorpus.serialize(FILE_NAME_OF_CORPUS+'.mm', corpus_memory_friendly)\n",
        "    return dictionary, corpus_memory_friendly\n",
        "\n",
        "## functions used in PyMallet_LDA\n",
        "\"\"\" \n",
        "    Here we are used the LDA implementation from GitHub PyMallet at:\n",
        "    https://github.com/mimno/PyMallet\n",
        "    The LDA code below is based on their lda_reference.py code written in Python\n",
        "    The PyMallet project has an MIT License see below.\n",
        "\n",
        "    INPUT FILES:\n",
        "    Previously created preprocessed chat corpus from either:\n",
        "    1) wholeChatsFilePOS_N_ADJ_V.csv -- preprocessing keeping nouns, adjectives, and verbs\n",
        "    2) wholeChatsFilePOS_N_ADJ.csv -- preprocessing keeping nouns and adjectives\n",
        "    3) wholeChatsFile.csv -- NO POS preprocessing so all parts of speech\n",
        "    4) onlyQuestionsFile.csv -- Only initial question of chats\n",
        "\n",
        "    OUTPUT FILES:\n",
        "    1) \"raw_\" text (.txt) file listing topics with each word scored\n",
        "    2) \"PyMallet_LDA_\" text (.txt) file containing only the text for the\n",
        "       specified number of topics with the specified number of words per topic\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 mimno\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import re, sys, random, math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from time import time\n",
        "\n",
        "def sample(documents, vocabulary_size, word_topics, topic_totals, word_counts, num_iterations, n_topics, doc_smoothing = 0.5, word_smoothing = 0.01):\n",
        "    smoothing_times_vocab_size = word_smoothing * vocabulary_size\n",
        "\n",
        "    word_pattern = re.compile(\"\\w[\\w\\-\\']*\\w|\\w\")\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        \n",
        "        for document in documents:\n",
        "            \n",
        "            doc_topic_counts = document[\"topic_counts\"]\n",
        "            token_topics = document[\"token_topics\"]\n",
        "            doc_length = len(token_topics)\n",
        "            for token_topic in token_topics:\n",
        "                \n",
        "                w = token_topic[\"word\"]\n",
        "                old_topic = token_topic[\"topic\"]\n",
        "                word_topic_counts = word_topics[w]\n",
        "                \n",
        "                ## erase the effect of this token\n",
        "                word_topic_counts[old_topic] -= 1\n",
        "                topic_totals[old_topic] -= 1\n",
        "                doc_topic_counts[old_topic] -= 1\n",
        "                \n",
        "                ###\n",
        "                ### SAMPLING DISTRIBUTION\n",
        "                ###\n",
        "                \n",
        "                ## Does this topic occur often in the document?\n",
        "                topic_probs = (doc_topic_counts + doc_smoothing) / (doc_length + n_topics * doc_smoothing)\n",
        "                ## Does this word occur often in the topic?\n",
        "                topic_probs *= (word_topic_counts + word_smoothing) / (topic_totals + smoothing_times_vocab_size)\n",
        "                \n",
        "                ## sample from an array that doesn't sum to 1.0\n",
        "                sample = random.uniform(0, np.sum(topic_probs))\n",
        "                \n",
        "                new_topic = 0\n",
        "                while sample > topic_probs[new_topic]:\n",
        "                    sample -= topic_probs[new_topic]\n",
        "                    new_topic += 1\n",
        "                \n",
        "                ## add back in the effect of this token\n",
        "                word_topic_counts[new_topic] += 1\n",
        "                topic_totals[new_topic] += 1\n",
        "                doc_topic_counts[new_topic] += 1\n",
        "                \n",
        "                token_topic[\"topic\"] = new_topic               \n",
        "\n",
        "def entropy(p):\n",
        "    ## make sure the vector is a valid probability distribution\n",
        "    p = p / np.sum(p)\n",
        "    \n",
        "    result = 0.0\n",
        "    for x in p:\n",
        "        if x > 0.0:\n",
        "            result += -x * math.log2(x)\n",
        "            \n",
        "    return result\n",
        "\n",
        "def print_topic(topic):\n",
        "    sorted_words = sorted(vocabulary, key=lambda w: word_topics[w][topic], reverse=True)\n",
        "    \n",
        "    for i in range(20):\n",
        "        w = sorted_words[i]\n",
        "        print(\"{}\\t{}\".format(word_topics[w][topic], w))\n",
        "\n",
        "def print_all_topics():\n",
        "    for topic in range(NUMBER_OF_TOPICS_PRINTED):\n",
        "        sorted_words = sorted(vocabulary, key=lambda w: word_topics[w][topic], reverse=True)\n",
        "        print(\" \".join(sorted_words[:20]))\n",
        "\n",
        "\n",
        "def PyMallet_LDA(docs, n_topics, stoplist = set()):\n",
        "    word_pattern = re.compile(\"\\w[\\w\\-\\']*\\w|\\w\")\n",
        "    word_counts = Counter()\n",
        "\n",
        "    documents = []\n",
        "    word_topics = {}\n",
        "    topic_totals = np.zeros(n_topics)\n",
        "\n",
        "\n",
        "    for line in docs:\n",
        "        #line = line.lower()\n",
        "        \n",
        "        tokens = word_pattern.findall(line)\n",
        "        \n",
        "        ## remove stopwords, short words, and upper-cased words\n",
        "        tokens = [w for w in tokens if not w in stoplist and len(w) >= 3 and not w[0].isupper()]\n",
        "        word_counts.update(tokens)\n",
        "        \n",
        "        doc_topic_counts = np.zeros(n_topics)\n",
        "        token_topics = []\n",
        "        \n",
        "        for w in tokens:\n",
        "            \n",
        "            ## Generate a topic randomly\n",
        "            topic = random.randrange(n_topics)\n",
        "            token_topics.append({ \"word\": w, \"topic\": topic })\n",
        "            \n",
        "            ## If we haven't seen this word before, initialize it\n",
        "            if not w in word_topics:\n",
        "                word_topics[w] = np.zeros(n_topics)\n",
        "            \n",
        "            ## Update counts: \n",
        "            word_topics[w][topic] += 1\n",
        "            topic_totals[topic] += 1\n",
        "            doc_topic_counts[topic] += 1\n",
        "        \n",
        "        documents.append({ \"original\": line, \"token_topics\": token_topics, \"topic_counts\": doc_topic_counts })\n",
        "\n",
        "    ## Now that we're done reading from disk, we can count the total\n",
        "    ##  number of words.\n",
        "    vocabulary = list(word_counts.keys())\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    num_iterations = 100\n",
        "    sample(documents, vocabulary_size, word_topics, topic_totals, word_counts, num_iterations, n_topics, doc_smoothing = 0.5, word_smoothing = 0.01)\n",
        "\n",
        "    return vocabulary, word_topics\n",
        "\n",
        "def write_PyMallet_LDA(vocabulary, word_topics, n_topics, n_words_per_topic, fileName):\n",
        "    \"\"\" Writes the results of PyMallet LDA to files and returns the resulting topics as\n",
        "        stings in topicList.\n",
        "    \"\"\"\n",
        "    outputFile = open(fileName, 'w')\n",
        "    outputFile.write(\"File: \" + fileName +\"\\n\\n\")\n",
        "    rawFileName = \"raw_\"+fileName\n",
        "    outputFileRaw = open(rawFileName, 'w')\n",
        "    outputFileRaw.write(\"File: \" + rawFileName +\"\\n\\n\")\n",
        "    topicList = []\n",
        "    for topic in range(n_topics):\n",
        "        sorted_words = sorted(vocabulary, key=lambda w: word_topics[w][topic], reverse=True)\n",
        "        topicStr = \" \".join(sorted_words[:n_words_per_topic])\n",
        "        topicList.append(topicStr)\n",
        "        outputFile.write(topicStr+\"\\n\")\n",
        "        outputFileRaw.write(topicStr+\"\\n\")\n",
        "        #print(topicStr)\n",
        "        for i in range(n_words_per_topic):\n",
        "            w = sorted_words[i]\n",
        "            #print(\"{}\\t{}\".format(word_topics[w][topic], w))\n",
        "            outputFileRaw.write(\"{}\\t{}\".format(word_topics[w][topic], w) +\"\\n\")\n",
        "        \n",
        "    outputFile.close()\n",
        "    outputFileRaw.close()\n",
        "    return topicList\n",
        "\n",
        "## functions used in the topic coherence metric calculations\n",
        "import math\n",
        "\n",
        "EPSILON = 0.000000001\n",
        "\n",
        "def calculateTopicCoherenceMetrics(documentsList, topicsList, stopWordDict = {}):\n",
        "    \"\"\" Calculates and returns the topic coherence metrics: averagePMI, averageLCP, averageNZ\n",
        "        for the set of topics in topicsList and the reference corpus in documentsList\n",
        "    \"\"\"\n",
        "    outputFileName = \"TC_metrics_.txt\"\n",
        "\n",
        "    coOccurrenceDict = {}\n",
        "    wordDict = {}\n",
        "    topicsList, topicsCoOccurrenceList, coOccurrenceDict, wordDict = findcoOoccurrencesAndWordsInTopics(topicsList)\n",
        "\n",
        "    numberOfTopics = len(topicsList)\n",
        "    \n",
        "    docCount = tallycoOoccurrencesAndWordsInDocs(documentsList, coOccurrenceDict, wordDict)\n",
        "\n",
        "    makeProbabilities(docCount, coOccurrenceDict, wordDict)\n",
        "\n",
        "    outputFile = open(outputFileName, 'w')\n",
        "\n",
        "    outputFile.write(\"File: \"+outputFileName+\"\\n\\n\")\n",
        "    \n",
        "    sumPMI = 0.0\n",
        "    sumLCP = 0.0\n",
        "    sumNZ = 0\n",
        "    index = 0\n",
        "    for topicCoOccurrence in topicsCoOccurrenceList:\n",
        "        topicPMI = calculateTopicPMI(topicCoOccurrence, coOccurrenceDict, wordDict)\n",
        "        topicLCP = calculateTopicLCP(topicCoOccurrence, coOccurrenceDict, wordDict)\n",
        "        topicNZ = calculateTopicNZ(topicCoOccurrence, coOccurrenceDict)\n",
        "        outputFile.write(topicsList[index]+\"\\n\")\n",
        "        outputFile.write(\"PMI = %.3f  \" % (topicPMI))\n",
        "        outputFile.write(\"LCP = %.3f  \" % (topicLCP))\n",
        "        outputFile.write(\"NZ = %d\\n\" % (topicNZ))\n",
        "        sumPMI += topicPMI\n",
        "        sumLCP += topicLCP\n",
        "        sumNZ += topicNZ\n",
        "        index += 1\n",
        "    averagePMI = sumPMI/numberOfTopics\n",
        "    averageLCP = sumLCP/numberOfTopics\n",
        "    averageNZ = sumNZ/numberOfTopics\n",
        "    outputFile.write(\"\\nAverage PMI of all topics: %.3f\\n\" % (averagePMI))\n",
        "    outputFile.write(\"\\nAverage LCP of all topics: %.3f\\n\" % (averageLCP))\n",
        "    outputFile.write(\"\\nAverage NZ of all topics: %.3f\\n\" % (averageNZ))\n",
        "    outputFile.close()\n",
        "    return averagePMI, averageLCP, averageNZ\n",
        "\n",
        "def makeProbabilities(docCount, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Converses the raw counts in the coOccurrenceDict and wordDict into probabilities.\"\"\"\n",
        "    for coOccurrence in coOccurrenceDict:\n",
        "        coOccurrenceDict[coOccurrence] /= float(docCount)\n",
        "    for word in wordDict:\n",
        "        wordDict[word] /= float(docCount)\n",
        "\n",
        "def calculateTopicPMI(topicCoOccurrenceList, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Calculates and returns a topic's total PMI. \"\"\" \n",
        "    sumPMI = 0.0\n",
        "    for topicCoOccurrence in topicCoOccurrenceList:\n",
        "        sumPMI += calculatePMI(topicCoOccurrence, coOccurrenceDict, wordDict)\n",
        "    return sumPMI/len(topicCoOccurrenceList)\n",
        "\n",
        "def calculateTopicLCP(topicCoOccurrenceList, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Calculates and returns a topic's total LCP. \"\"\" \n",
        "    sumLCP = 0.0\n",
        "    for topicCoOccurrence in topicCoOccurrenceList:\n",
        "        firstWord, secondWord = topicCoOccurrence\n",
        "        sumLCP += calculateLCP(firstWord, topicCoOccurrence, coOccurrenceDict, wordDict)\n",
        "        sumLCP += calculateLCP(secondWord, topicCoOccurrence, coOccurrenceDict, wordDict)\n",
        "    return sumLCP/(2*len(topicCoOccurrenceList))\n",
        "\n",
        "def calculateTopicNZ(topicCoOccurrenceList, coOccurrenceDict):\n",
        "    \"\"\" Calculates and returns a topic's total NZ. \"\"\" \n",
        "    sumNZ = 0\n",
        "    for topicCoOccurrence in topicCoOccurrenceList:\n",
        "        if coOccurrenceDict[topicCoOccurrence] == 0.0:\n",
        "            sumNZ += 1\n",
        "    return sumNZ\n",
        "\n",
        "def calculatePMI(topicCoOccurrence, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Calculates and returns the PMI for a pair of words in the topicCoOccurrence tuple. \"\"\"\n",
        "    wordI, wordJ = topicCoOccurrence\n",
        "    PMI = math.log((coOccurrenceDict[topicCoOccurrence]+EPSILON)/(wordDict[wordI]*wordDict[wordJ]),10)\n",
        "    return PMI\n",
        "        \n",
        "        \n",
        "def calculateLCP(word, topicCoOccurrence, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Calculates and returns the LCP for a word in the pair of words in the topicCoOccurrence tuple. \"\"\"\n",
        "    LCP = math.log((coOccurrenceDict[topicCoOccurrence]+EPSILON)/(wordDict[word]),10)\n",
        "    return LCP\n",
        "                \n",
        "def tallycoOoccurrencesAndWordsInDocs(documentsList, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Tallys across all the documents in documentsList the word pair co-occurrences in coOccurrenceDict, and\n",
        "        individual words in wordDict.\"\"\"\n",
        "    docCount = 0\n",
        "    for document in documentsList:\n",
        "        emptyDoc = tallyCoOccurrencesInDoc(document, coOccurrenceDict, wordDict)\n",
        "        if not emptyDoc:\n",
        "            docCount += 1\n",
        "    return docCount\n",
        "\n",
        "def tallyCoOccurrencesInDoc(document, coOccurrenceDict, wordDict):\n",
        "    \"\"\" Tallys for an individual document the word pair co-occurrences in coOccurrenceDict, and\n",
        "        individual words in wordDict.\"\"\"\n",
        "    docCoOccurrenceDict = {}\n",
        "    docWordDict = {}\n",
        "    \n",
        "    wordList = document.strip().split()\n",
        "    if len(wordList) == 0:\n",
        "        return True   # empty document\n",
        "    \n",
        "    # eliminate duplicate words by converting to a set and back\n",
        "    wordSet = set(wordList)\n",
        "    wordList = list(wordSet)\n",
        "\n",
        "    wordList.sort()\n",
        "    for first in range(len(wordList)):\n",
        "        if wordList[first] in wordDict:\n",
        "            wordDict[wordList[first]] += 1\n",
        "        for second in range(first+1,len(wordList)):\n",
        "            coOccurrenceTuple = (wordList[first], wordList[second])\n",
        "            if coOccurrenceTuple in coOccurrenceDict:\n",
        "                coOccurrenceDict[coOccurrenceTuple] += 1\n",
        "    return False   # not empty document\n",
        "\n",
        "def findcoOoccurrencesAndWordsInTopics(topicsList):\n",
        "    \"\"\" Processes the topics file and returns:\n",
        "        topicsList - list of strings with one whole topic as a string,\n",
        "        topicsCoOccurrenceList - a list-of-lists with the inner-list being the list word pairs as tuples within a topic,\n",
        "        coOccurrenceDict - keys are tuple of word pairs that co-occur in the topics with their associated values of 0,\n",
        "        wordDict - keys are words that occur in the topics with their associate values of 0.\"\"\"\n",
        "\n",
        "    topicsCoOccurrenceList = []\n",
        "    coOccurrenceDict = {}\n",
        "    wordDict = {}\n",
        "    topicTupleList = []\n",
        "    for line in topicsList:\n",
        "        topicTupleList = []\n",
        "        wordList = line.strip().split()\n",
        "        wordList.sort()\n",
        "        for first in range(len(wordList)):\n",
        "            wordDict[wordList[first]] = 0\n",
        "            for second in range(first+1,len(wordList)):\n",
        "                coOccurrenceTuple = (wordList[first], wordList[second])\n",
        "                coOccurrenceDict[coOccurrenceTuple] = 0\n",
        "                topicTupleList.append(coOccurrenceTuple)\n",
        "        topicsCoOccurrenceList.append(topicTupleList)\n",
        "    return topicsList, topicsCoOccurrenceList,coOccurrenceDict, wordDict\n",
        "\n",
        "def tallyTriOccurrencesInWindow(document, windowSize, triOccurrenceDict, wordFreqDict, stopWordDict):\n",
        "    \"\"\" Tally the tri-occurrences of non-stop words in all documents of a given window size. \"\"\"\n",
        "    wordList = document\n",
        "\n",
        "    initialChuckSize = min(len(wordList), windowSize)\n",
        "\n",
        "    # process initial window size or whole line if it is smaller than window size\n",
        "    for first in range(initialChuckSize-2):\n",
        "        if wordList[first] in wordFreqDict:\n",
        "            wordFreqDict[wordList[first]] += 1\n",
        "        else:\n",
        "            wordFreqDict[wordList[first]] = 1\n",
        "            \n",
        "        for second in range(first+1,initialChuckSize-1):          \n",
        "            for third in range(second+1,initialChuckSize):          \n",
        "                if wordList[first] != wordList[second] and \\\n",
        "                   wordList[first] != wordList[third] and \\\n",
        "                   wordList[second] != wordList[third] and \\\n",
        "                   wordList[first] not in stopWordDict and \\\n",
        "                   wordList[second] not in stopWordDict and \\\n",
        "                   wordList[third] not in stopWordDict:\n",
        "                    words = [wordList[first],wordList[second],wordList[third]]\n",
        "                    words.sort()\n",
        "                    triOccurrenceTuple = (words[0], words[1], words[2])\n",
        "                    if triOccurrenceTuple in triOccurrenceDict:\n",
        "                        triOccurrenceDict[triOccurrenceTuple] += 1\n",
        "                    else:\n",
        "                        triOccurrenceDict[triOccurrenceTuple] = 1\n",
        "\n",
        "    # slide the window down the whole length of the line\n",
        "    for nextWordIndex in range(windowSize, len(wordList)):\n",
        "        if wordList[nextWordIndex] in wordFreqDict:\n",
        "            wordFreqDict[wordList[nextWordIndex]] += 1\n",
        "        else:\n",
        "            wordFreqDict[wordList[nextWordIndex]] = 1\n",
        "        for second in range(nextWordIndex -1, nextWordIndex-windowSize+2, -1):\n",
        "            for third in range(second-1,nextWordIndex-windowSize+1, -1):          \n",
        "                if wordList[nextWordIndex] != wordList[second] and \\\n",
        "                   wordList[nextWordIndex] != wordList[third] and \\\n",
        "                   wordList[second] != wordList[third] and \\\n",
        "                   wordList[nextWordIndex] not in stopWordDict and \\\n",
        "                   wordList[second] not in stopWordDict and \\\n",
        "                   wordList[third] not in stopWordDict:\n",
        "                    words = [wordList[nextWordIndex],wordList[second],wordList[third]]\n",
        "                    words.sort()\n",
        "                    triOccurrenceTuple = (words[0], words[1], words[2])\n",
        "                    if triOccurrenceTuple in triOccurrenceDict:\n",
        "                        triOccurrenceDict[triOccurrenceTuple] += 1\n",
        "                    else:\n",
        "                        triOccurrenceDict[triOccurrenceTuple] = 1\n",
        "\n",
        "def tallyCoOccurrencesInWindow(document, windowSize, coOccurrenceDict, wordFreqDict, stopWordDict):\n",
        "    \"\"\" Tally the co-occurrences of non-stop words in all documents of a given window size. \"\"\"\n",
        "    wordList = document\n",
        "\n",
        "    initialChuckSize = min(len(wordList), windowSize)\n",
        "\n",
        "    # process initial window size or whole line if it is smaller than window size\n",
        "    for first in range(initialChuckSize):\n",
        "        if wordList[first] in wordFreqDict:\n",
        "            wordFreqDict[wordList[first]] += 1\n",
        "        else:\n",
        "            wordFreqDict[wordList[first]] = 1\n",
        "            \n",
        "        for second in range(first+1,initialChuckSize):          \n",
        "            if wordList[first] != wordList[second] and \\\n",
        "               wordList[first] not in stopWordDict and \\\n",
        "               wordList[second] not in stopWordDict:\n",
        "                if wordList[first] < wordList[second]:\n",
        "                    coOccurrenceTuple = (wordList[first], wordList[second])\n",
        "                elif wordList[first] > wordList[second]:\n",
        "                    coOccurrenceTuple = (wordList[second], wordList[first])\n",
        "                if coOccurrenceTuple in coOccurrenceDict:\n",
        "                    coOccurrenceDict[coOccurrenceTuple] += 1\n",
        "                else:\n",
        "                    coOccurrenceDict[coOccurrenceTuple] = 1\n",
        "\n",
        "    # slide the window down the whole length of the line\n",
        "    for nextWordIndex in range(windowSize, len(wordList)):\n",
        "        if wordList[nextWordIndex] in wordFreqDict:\n",
        "            wordFreqDict[wordList[nextWordIndex]] += 1\n",
        "        else:\n",
        "            wordFreqDict[wordList[nextWordIndex]] = 1\n",
        "        for otherWordIndex in range(nextWordIndex-windowSize+1, nextWordIndex):\n",
        "            if wordList[nextWordIndex] != wordList[otherWordIndex] and \\\n",
        "               wordList[nextWordIndex] not in stopWordDict and \\\n",
        "               wordList[otherWordIndex] not in stopWordDict:\n",
        "                if wordList[nextWordIndex] < wordList[otherWordIndex]:\n",
        "                    coOccurrenceTuple = (wordList[nextWordIndex], wordList[otherWordIndex])\n",
        "                elif wordList[nextWordIndex] > wordList[otherWordIndex]:\n",
        "                    coOccurrenceTuple = (wordList[otherWordIndex], wordList[nextWordIndex])\n",
        "                if coOccurrenceTuple in coOccurrenceDict:\n",
        "                    coOccurrenceDict[coOccurrenceTuple] += 1\n",
        "                else:\n",
        "                    coOccurrenceDict[coOccurrenceTuple] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z-g07C4DiG-5",
        "outputId": "ffcc4c3b-37e9-4129-a12a-5251858a1b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Phase 2 which runs the unsupervised topic modeling techniques. \n",
            "\n",
            "You should have first run Phase 1 to pre-process your chat data. \n",
            "It would generate cleaned chat files varying the parts of speech or question-only. \n",
            "Files generated are: wholeChatsFile.txt, wholeChatsFilePOS_N_ADJ_V.txt, \n",
            "wholeChatsFilePOS_N_ADJ.txt, and onlyQuestionsFile.txt.\n",
            "\n",
            "\n",
            "Step 1. Please input the pre-processed (.txt) file.\n",
            "(For example: \"wholeChatsFile.txt\"): /content/wholeChatsFile.txt\n",
            "\n",
            "Step 2. Please specify the number of topics. (suggested range 10-20)\n",
            " 10\n",
            "\n",
            "Step 3. Please specify the number of words per topics. (suggested range 5-10)\n",
            " 5\n",
            "===================================\n",
            "\n",
            "Performing PyMallet LDA topic modeling -- please wait it might take a couple minutes!\n",
            "\n",
            "Results for PyMallet LDA  TC-PMI 0.000, TC-LCP 0.000, TC-NZ 0.000:\n",
            "rod how book would link\n",
            "rod book check article study\n",
            "will day find check article\n",
            "how search rod online work\n",
            "what access article send answer\n",
            "find article link moment would\n",
            "rod how would link place\n",
            "ill journal question will how\n",
            "time article check book rod\n",
            "rod source search how today\n",
            "===================================\n",
            "\n",
            "Performing LDA topic modeling -- please wait it might take a couple minutes!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results for LDA  TC-PMI 0.000, TC-LCP 0.000, TC-NZ 0.000:\n",
            "rod article check search book\n",
            "rod article search book check\n",
            "rod article book link wa\n",
            "rod article book search check\n",
            "rod article search link work\n",
            "rod article search book check\n",
            "rod article link check search\n",
            "rod article book search check\n",
            "rod book article link search\n",
            "rod search book link article\n",
            "===================================\n",
            "\n",
            "Performing TF-IDF & LSA topic modeling -- please wait it might take a couple minutes!\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b8aeddac0043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b8aeddac0043>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"topic modeling -- please wait it might take a couple minutes!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mtopicList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_per_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0maveragePMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageLCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageNZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateTopicCoherenceMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nResults for\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" TC-PMI %3.3f, TC-LCP %3.3f, TC-NZ %3.3f:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maveragePMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageLCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageNZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-b8aeddac0043>\u001b[0m in \u001b[0;36mrun_TFIDF_LSA\u001b[0;34m(documents, n_topics, n_words_per_topic)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Initialize an LSI transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mnumberOfTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;31m# (recommended between 200-500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mlsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumberOfTopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;31m# create a double wrapper over chat corpus: bow -> tfidf -> fold-in-lsi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mcorpus_lsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, corpus, chunksize, decay)\u001b[0m\n\u001b[1;32m    506\u001b[0m                         update = Projection(\n\u001b[1;32m    507\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                             \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                         )\n\u001b[1;32m    510\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, m, k, docs, use_svdlibc, power_iters, extra_dims, dtype)\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                     extra_dims=self.extra_dims, dtype=dtype)\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36mstochastic_svd\u001b[0;34m(corpus, rank, num_terms, chunksize, extra_dims, power_iters, dtype, eps)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"orthonormalizing %s action matrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr_destroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# orthonormalize the range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"running %i power iterations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mqr_destroy\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"computing QR of %s dense matrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0mgeqrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geqrf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m     \u001b[0mqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeqrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m     \u001b[0mqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeqrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0ma\u001b[0m  \u001b[0;31m# free up mem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: On entry to DGEQRF parameter number 4 had an illegal value"
          ]
        }
      ],
      "source": [
        "\"\"\" File:  P2_unsupervised_topic_modeling.py  \n",
        "\n",
        "    Description:  Loads a previously created preprocessed chat corpus, then performs\n",
        "    topic modeling utilizing unsupervised techniques of:\n",
        "    1) Latent Semantic Analysis (TF-IDF & LSA) using gensim\n",
        "    2) probabilistic Latent Semantic Analysis (TF-IDF & pLSA) using gensim\n",
        "    3) Latent Dirichlet Allocation (LDA) using scikit-learn.org (sklearn) LDA module\n",
        "    4) Latent Dirichlet Allocation (LDA), PyMallet\n",
        "    Here we are used the LDA implementation from GitHub PyMallet at:\n",
        "    https://github.com/mimno/PyMallet\n",
        "    The LDA code below is based on their lda_reference.py code written in Python\n",
        "    The PyMallet project has an MIT License see below.\n",
        "================================================================================\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 mimno\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "===========================================================================\n",
        "    INPUT FILES:  User inputs file to process\n",
        "    Previously created preprocessed chat corpus from P1_preprocess_data.py either:\n",
        "    1) wholeChatsFilePOS_N_ADJ_V.txt -- preprocessing keeping nouns, adjectives, and verbs\n",
        "    2) wholeChatsFilePOS_N_ADJ.txt -- preprocessing keeping nouns and adjectives\n",
        "    3) wholeChatsFile.txt -- NO POS preprocessing so all parts of speech\n",
        "    4) onlyQuestionsFile.txt -- Only initial question of chats\n",
        "\n",
        "    OUTPUT FILES for each of the 4 unsupervised topic modeling techniques:\n",
        "    1) \"raw_\" text (.txt) file listing topics with each word scored\n",
        "    2) \"LDA_\" text (.txt) file containing only the text for the\n",
        "       specified number of topics with the specified number of words per topic\n",
        "\n",
        "    OUTPUT FILES for to aid the semi-supervised topic modeling techniques of Phase 3:\n",
        "    1) possible_2_word_anchors.txt most frequent 2-word occurrence across combined topics\n",
        "       of all four unsupervised topic modeling techniques\n",
        "    2) possible_3_word_anchors.txt most frequent 3-word occurrence across combined topics\n",
        "       of all four unsupervised topic modeling techniques      \n",
        "    \n",
        "\"\"\"\n",
        "import os.path\n",
        "from pprint import pprint  # pretty-printer\n",
        "from collections import defaultdict\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from time import time\n",
        "\n",
        "#from P2_utility_functions import *\n",
        "\n",
        "def main():\n",
        "    # ask users to input the name of the csv file cleaned, make sure it contains the column of 'body'\n",
        "    print('Welcome to Phase 2 which runs the unsupervised topic modeling techniques.',\n",
        "          '\\n\\nYou should have first run Phase 1 to pre-process your chat data.',\n",
        "          '\\nIt would generate cleaned chat files varying the parts of speech or question-only.',\n",
        "          '\\nFiles generated are: wholeChatsFile.txt, wholeChatsFilePOS_N_ADJ_V.txt,',\n",
        "          '\\nwholeChatsFilePOS_N_ADJ.txt, and onlyQuestionsFile.txt.\\n')\n",
        "\n",
        "    prompt = \"\\nStep 1. Please input the pre-processed (.txt) file.\" + \\\n",
        "             '\\n(For example: \"wholeChatsFile.txt\"):'\n",
        "    fileName = getFileName(prompt)\n",
        "    chats = readChatCorpusFile(fileName)\n",
        "\n",
        "    modelDict = {'PyMallet LDA':run_PyMallet_LDA, 'LDA':runLDA,\n",
        "                 'TF-IDF & LSA':run_TFIDF_LSA, 'TF-IDF & pLSA':run_TFIDF_pLSA}\n",
        "\n",
        "    n_topics = getPositiveInteger('\\nStep 2. Please specify the number of topics. (suggested range 10-20)\\n')\n",
        "    n_words_per_topic = getPositiveInteger('\\nStep 3. Please specify the number of words per topics. (suggested range 5-10)\\n')\n",
        "\n",
        "    combinedTopicsAcrossAllTechniques = []\n",
        "    for model in modelDict:\n",
        "        print(\"=\"*35)\n",
        "        print(\"\\nPerforming\", model,\"topic modeling -- please wait it might take a couple minutes!\")\n",
        "        topicList = modelDict[model](chats, n_topics, n_words_per_topic)\n",
        "        averagePMI, averageLCP, averageNZ = calculateTopicCoherenceMetrics(chats, topicList)\n",
        "        print(\"\\nResults for\",model,\" TC-PMI %3.3f, TC-LCP %3.3f, TC-NZ %3.3f:\" % (averagePMI, averageLCP, averageNZ))\n",
        "        for topic in topicList:\n",
        "            print(topic)\n",
        "        combinedTopicsAcrossAllTechniques.extend(topicList)\n",
        "\n",
        "    # generate files of possible anchors for semi-supervised topic modeling techniques\n",
        "    coOccurrenceDict, triOccurrenceDict = generate_Co_and_Tri_occurrence_dictionary(combinedTopicsAcrossAllTechniques,n_words_per_topic)\n",
        "    writeOccurrenceFile(2, coOccurrenceDict)\n",
        "    writeOccurrenceFile(3, triOccurrenceDict)\n",
        "\n",
        "def runLDA(documents,n_topics, n_words_per_topic, max_features=1000, stop_words='english'):\n",
        "    \"\"\" Performs LDA topic modeling and return resulting topics as strings in topicList \"\"\"\n",
        "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
        "    tf_vectorizer = CountVectorizer(max_features=max_features, stop_words='english')\n",
        "    tf = tf_vectorizer.fit_transform(documents)\n",
        "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "    # Fit the LDA model\n",
        "    lda_model = LatentDirichletAllocation(n_topics, max_iter=50, learning_method='online',\n",
        "                                    learning_decay = 0.7,\n",
        "                                    learning_offset=50.,\n",
        "                                    random_state=0)\n",
        "    lda_fit = lda_model.fit(tf)\n",
        "    lda_output = lda_model.transform(tf)\n",
        "\n",
        "    fileName = \"LDA_\"+\"_\"+str(n_topics)+\"topics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "    topicList = write_file_top_words(lda_fit, tf_feature_names, n_words_per_topic, fileName)\n",
        "    return topicList\n",
        "\n",
        "def run_TFIDF_pLSA(documents,n_topics, n_words_per_topic, max_features=1000, stop_words='english'):\n",
        "    \"\"\" Performs TF-IDF and pLSA topic modeling and return resulting topics as strings in topicList \"\"\"\n",
        "    # Vectorize raw documents to tf-idf matrix: \n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
        "    tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "    nmf = NMF(n_components=n_topics, random_state=1,\n",
        "              beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "              l1_ratio=.5).fit(tfidf)\n",
        "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "    fileName = \"TFIDF_pLSA_\"+str(n_topics)+\"topics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "    topicList = write_file_top_words(nmf, tfidf_feature_names, n_words_per_topic, fileName)\n",
        "    return topicList\n",
        "\n",
        "def run_PyMallet_LDA(documents, n_topics, n_words_per_topic, fileNameCorpus=\"\"):\n",
        "    \"\"\" Performs PyMallet LDA topic modeling and return resulting topics as strings in topicList \"\"\"\n",
        "    vocabulary, word_topics = PyMallet_LDA(documents, n_topics)\n",
        "    fileName = \"PyMallet_LDA_\"+fileNameCorpus+\"_\"+str(n_topics) \\\n",
        "               +\"topics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "\n",
        "    topicList = write_PyMallet_LDA(vocabulary, word_topics, n_topics, n_words_per_topic, fileName)\n",
        "\n",
        "    return topicList\n",
        "\n",
        "def run_TFIDF_LSA(documents,n_topics, n_words_per_topic):\n",
        "    \"\"\" Performs TF-IDF and LSA topic modeling and return resulting topics as strings in topicList \"\"\"\n",
        "    stoplist = set()  # preprocessing removed stop words already...\n",
        "    dictionary, corpus = createCorpusDictionary(documents, stoplist)\n",
        "    tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model\n",
        "\n",
        "    # Apply a transformation to a whole corpus \n",
        "    corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "    # Initialize an LSI transformation\n",
        "    numberOfTopics = 300 # (recommended between 200-500)\n",
        "    lsi = models.LsiModel(corpus_tfidf, id2word = dictionary, num_topics=numberOfTopics)\n",
        "    # create a double wrapper over chat corpus: bow -> tfidf -> fold-in-lsi\n",
        "    corpus_lsi = lsi[corpus_tfidf]\n",
        "    fileName = \"TFIDF_LSA_\"+str(n_topics)+\"topics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "    topicList = write_LSA(lsi, n_topics, n_words_per_topic, fileName)\n",
        "    return topicList\n",
        "    \n",
        "def generate_Co_and_Tri_occurrence_dictionary(combinedTopicsAcrossAllTechniques,n_words_per_topic):\n",
        "    \"\"\" To aid the semi-supervised topic modeling techniques of Phase 3, determines the\n",
        "        co-occurrences (2-words) and tri-occurrences across combined topics of all four\n",
        "        unsupervised topic modeling techniques.\n",
        "    \"\"\"\n",
        "    coOccurrenceDict = {}\n",
        "    triOccurrenceDict = {}\n",
        "    wordFreqDict = {}\n",
        "    wordFreqDict2 = {}\n",
        "    windowSize = n_words_per_topic\n",
        "    stopWordDict = {}  #stop words previously removed\n",
        "    combinedTopicsFile = open(\"combinedTopicsFile.txt\", 'w')\n",
        "    for topic in combinedTopicsAcrossAllTechniques:\n",
        "        document = topic.split()\n",
        "        tallyTriOccurrencesInWindow(document, windowSize, triOccurrenceDict, wordFreqDict, stopWordDict)\n",
        "        tallyCoOccurrencesInWindow(document, windowSize, coOccurrenceDict, wordFreqDict2, stopWordDict)\n",
        "        combinedTopicsFile.write(topic+\"\\n\")\n",
        "    combinedTopicsFile.close()\n",
        "    return coOccurrenceDict, triOccurrenceDict\n",
        "\n",
        "def writeOccurrenceFile(occurrenceSize, occurrenceDict):\n",
        "    \"\"\" Called twice to generate two files to aid Phase 3 semi-supervised topic modeling:\n",
        "        1) possible_2_word_anchors.txt most frequent 2-word occurrence across combined\n",
        "           topics of all four unsupervised topic modeling techniques, and\n",
        "        2) possible_3_word_anchors.txt most frequent 3-word occurrence across combined\n",
        "           topics of all four unsupervised topic modeling techniques.\n",
        "    \"\"\"\n",
        "    occurrencesFile = open(\"possible_\"+str(occurrenceSize)+\"_word_anchors.txt\", 'w')\n",
        "    occurrencesFile.write(\"Possible \"+str(occurrenceSize)+\" word anchors for semi-supervised topic modeling.\\n\")\n",
        "    occurrencesFile.write(\"Found from most frequently occuring \"+ str(occurrenceSize)+ \"-word occurrences from\\n\" +\n",
        "                          \"all topics found by supervised topic modeling techniques:\\n\" +\n",
        "                          \"LDA, PyMallet_LDA, pLSA, and LSA\\n\\n\")\n",
        "    countList = []\n",
        "    for wordTuple, count in occurrenceDict.items():\n",
        "        countList.append((count, wordTuple))\n",
        "    countList.sort()\n",
        "    countList.reverse()\n",
        "    numberToSee = min(len(countList), 50)\n",
        "    for index in range(numberToSee):\n",
        "        count, wordTuple = countList[index]\n",
        "        occurrencesFile.write(\"tuple count: %d  words %s\\n\" % (count, str(wordTuple)))\n",
        "    occurrencesFile.close()\n",
        "    \n",
        "\n",
        "main()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb_gL86_PhXB",
        "outputId": "c5a68c00-b868-422a-8ef8-9fc384aeafa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lda\n",
            "  Downloading lda-2.0.0-cp37-cp37m-manylinux1_x86_64.whl (351 kB)\n",
            "\u001b[K     |████████████████████████████████| 351 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from lda) (1.21.6)\n",
            "Collecting pbr<4,>=0.6\n",
            "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pbr, lda\n",
            "Successfully installed lda-2.0.0 pbr-3.1.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install corextopic\n",
        "#!pip install lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PAiA7eGDRVgb"
      },
      "outputs": [],
      "source": [
        "#!pip show lda\n",
        "\n",
        "#For Workaround: Drop the Python Files from GuidedLDA_WorkAround Git Repo in the lda folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HA8faS3wMeje"
      },
      "outputs": [],
      "source": [
        "#P3 utility functions\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" File:  P3_utility_functions.py  \n",
        "\n",
        "    Description:  Utility functions to performs\n",
        "    semi-supervised topic modeling utilizing CorEx and GuidedLDA.\n",
        "\n",
        "    Acknowledgements:\n",
        "\n",
        "    Here we are used the CorEx (Correlation Explanation) package available at GitHub:\n",
        "    https://github.com/gregversteeg/corex_topic\n",
        "\n",
        "    Here we are used the GuidedLDA package is available at GitHub:\n",
        "    https://github.com/vi3k6i5/GuidedLDA\n",
        "    NOTE:  We had difficulty installing GuidedLDA, but we were finally successful\n",
        "    by following the work-around posted at:\n",
        "    https://github.com/dex314/GuidedLDA_WorkAround\n",
        "\n",
        "\"\"\"\n",
        "import os.path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from corextopic import corextopic as ct\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from time import time\n",
        "\n",
        "import re, sys, random, math\n",
        "import numpy as np\n",
        "from lda import guidedlda as glda\n",
        "from lda import glda_datasets as gldad\n",
        "\n",
        "from collections import Counter\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "def readAnchorsFile(fileName):\n",
        "    \"\"\" Reads anchor/seeds from fileName and returns list-of-lists anchorList \"\"\"\n",
        "    anchorList = []\n",
        "    anchorFile = open(fileName, 'r')\n",
        "    for line in anchorFile:\n",
        "        wordList = line.strip().split()\n",
        "        if len(wordList) > 0:\n",
        "            anchorList.append(wordList)\n",
        "    anchorFile.close()\n",
        "\n",
        "    return anchorList\n",
        "\n",
        "def run_GuidedLDA(chats, anchorList, n_topics, n_words_per_topic,SEED_CONFIDENCE=0.75):\n",
        "    \"\"\" Perform GuidedLDA on corpus from chats using anchorList.\n",
        "        Returns topics as strings in topicList.\n",
        "    \"\"\"\n",
        "    word2id = {}\n",
        "    docs = []\n",
        "    id2word = {}\n",
        "    wordList = []\n",
        "    wordId = 0\n",
        "    for documentLine in chats:\n",
        "        newDoc = \"\"\n",
        "        for word in documentLine.split():\n",
        "            if word not in word2id:\n",
        "                word2id[word] = wordId\n",
        "                id2word[wordId] = word\n",
        "                wordList.append(word)\n",
        "                wordId += 1\n",
        "            newDoc += word + \" \"\n",
        "        if len(newDoc) > 0:\n",
        "            docs.append(newDoc)\n",
        "    numDocs = len(docs)\n",
        "    numWords = len(word2id)\n",
        "    vocab = tuple(wordList)\n",
        "\n",
        "    X = np.ndarray(shape=(numDocs, numWords), dtype=int)\n",
        "\n",
        "    word_counts = Counter()\n",
        "    documents = []\n",
        "    word_topics = {}\n",
        "    topic_totals = np.zeros(n_topics)\n",
        "\n",
        "    for docIndex, docLine in enumerate(docs):\n",
        "        \n",
        "        for word in docLine.strip().split():\n",
        "            wordId = word2id[word]\n",
        "            X[docIndex][wordId] += 1\n",
        "\n",
        "    seed_topic_list = anchorList\n",
        "    model = glda.GuidedLDA(n_topics=n_topics, n_iter=100,\n",
        "                           random_state=7, refresh=20)\n",
        "    seed_topics = {}\n",
        "    for t_id, st in enumerate(seed_topic_list):\n",
        "        for word in st:\n",
        "            seed_topics[word2id[word]] = t_id\n",
        "\n",
        "    model.fit(X, seed_topics=seed_topics, seed_confidence=SEED_CONFIDENCE)\n",
        "\n",
        "    # Display and write to file the results of CorEx with no anchors\n",
        "    fileName = \"GuidedLDA_seeds_\"+str(len(seed_topic_list))+\"_confidence_\"+ \\\n",
        "               str(SEED_CONFIDENCE)+\"_\"+str(n_topics) +\"topics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "    outputFile = open(fileName, 'w')\n",
        "    outputFile.write(\"File: \" + fileName +\"\\n\\n\")\n",
        "    topicList = []\n",
        "    topic_word = model.topic_word_\n",
        "    for i, topic_dist in enumerate(topic_word):\n",
        "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_words_per_topic+1):-1]\n",
        "        topicStr = '{}'.format(' '.join(topic_words))\n",
        "        topicList.append(topicStr)     \n",
        "        outputFile.write(topicStr+\"\\n\")\n",
        "    outputFile.close()\n",
        "    return topicList\n",
        "\n",
        "def run_CorEx(documents, anchorList, n_topics, n_words_per_topic):\n",
        "    \"\"\" Performs CorEx on corpus documents using anchorList.\n",
        "        Returns topics as strings in topicList.\n",
        "    \"\"\"\n",
        "    # CorEx uses an TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer(max_df=.5, min_df=10, max_features=None,\n",
        "    ##    ngram_range=(1, 2),  for bi-grams\n",
        "    ##    ngram_range=(1,3),   for bi-grams and tri-grams\n",
        "        ngram_range=(1,1),     # for no bi-grams or tri-grams\n",
        "        norm=None,\n",
        "        binary=True,\n",
        "        use_idf=False,\n",
        "        sublinear_tf=False\n",
        "    )\n",
        "\n",
        "    # Fit chat corpus to TF-IDF vectorization\n",
        "    vectorizer = vectorizer.fit(documents)\n",
        "    tfidf = vectorizer.transform(documents)\n",
        "    vocab = vectorizer.get_feature_names()\n",
        "\n",
        "    # Apply CorEx with no anchors for a comparison\n",
        "    anchors = []\n",
        "    model = ct.Corex(n_hidden=n_topics, seed=42) # n_hidden specifies the # of topics\n",
        "    model = model.fit(tfidf, words=vocab)\n",
        "\n",
        "    # Display and write to file the results of CorEx with no anchors\n",
        "    fileName = \"CorEx_no_anchors_\"+str(n_topics)+\"topoics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "    outputFile = open(fileName, 'w')\n",
        "    outputFile.write(\"File: \" + fileName +\"\\n\\n\")\n",
        "\n",
        "    print(\"\\nCorEx Topics with no anchors:\")\n",
        "    for i, topic_ngrams in enumerate(model.get_topics(n_words=n_words_per_topic)):\n",
        "        topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "        print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))\n",
        "        outputFile.write(\"{}\".format(\" \".join(topic_ngrams))+\"\\n\")\n",
        "    outputFile.close()\n",
        "\n",
        "    ## remove anchor words that are not in the chat corpus\n",
        "    anchors = [\n",
        "        [a for a in topic if a in vocab]\n",
        "        for topic in anchorList\n",
        "    ]\n",
        "\n",
        "    model = ct.Corex(n_hidden=n_topics, seed=42)\n",
        "    model = model.fit(\n",
        "        tfidf,\n",
        "        words=vocab,\n",
        "        anchors=anchors, # Pass the anchors in here\n",
        "        anchor_strength=3 # Tell the model how much it should rely on the anchors\n",
        "    )\n",
        "\n",
        "    # Display and write to file the results of CorEx with no anchors\n",
        "    fileName = \"CorEx_anchors_\"+str(len(anchors))+\"_\"+str(n_topics) \\\n",
        "               +\"topoics_\"+str(n_words_per_topic)+\"words.txt\"\n",
        "    outputFile = open(fileName, 'w')\n",
        "    outputFile.write(\"File: \" + fileName +\"\\n\\n\")\n",
        "    topicList = []\n",
        "    print(\"\\nCorEx Topics with anchors:\")\n",
        "    for i, topic_ngrams in enumerate(model.get_topics(n_words=n_words_per_topic)):\n",
        "        topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
        "        topicList.append(\" \".join(topic_ngrams))\n",
        "        print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))\n",
        "        outputFile.write(\"{}\".format(\" \".join(topic_ngrams))+\"\\n\")\n",
        "    outputFile.close()    \n",
        "    return topicList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "D78FgxDoMiUE",
        "outputId": "d01136f6-ae95-4627-e55e-02eb35d995bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Phase 3 which runs the semi-supervised topic modeling techniques. \n",
            "\n",
            "You should have first run Phase 1 to pre-process your chat data. \n",
            "It would generate cleaned chat files varying the parts of speech or question-only. \n",
            "Files generated are: wholeChatsFile.txt, wholeChatsFilePOS_N_ADJ_V.txt, \n",
            "wholeChatsFilePOS_N_ADJ.txt, and onlyQuestionsFile.txt.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You could have also run Phase 2 to execute unsupervised topic modeling techniques. \n",
            "It would generate files: possible_2_word_anchors.txt and possible_3_word_anchors.txt which \n",
            "you might use to create a text-file (.txt) with anchors one per line.\n",
            "\n",
            "\n",
            "Step 1. Please input the pre-processed (.txt) file.\n",
            "(For example: \"wholeChatsFile.txt\"): /content/wholeChatsFile.txt\n",
            "\n",
            "Step 2. Please input the anchors/seeds (.txt) file.\n",
            "(For example: \"anchors.txt\"): /content/anchors.txt\n",
            "\n",
            "Step 3. Please specify the number of topics. (suggested range 10-20)\n",
            " 10\n",
            "\n",
            "Step 4. Please specify the number of words per topics. (suggested range 5-10)\n",
            " 5\n",
            "===================================\n",
            "\n",
            "Performing GuidedLDA topic modeling -- please wait it might take a couple minutes!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:lda:all zero column in document-term matrix found\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-683acee9d461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-683acee9d461>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"topic modeling -- please wait it might take a couple minutes!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mtopicList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchorList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_per_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0maveragePMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageLCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageNZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateTopicCoherenceMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nResults for\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" TC-PMI %3.3f, TC-LCP %3.3f, TC-NZ %3.3f:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maveragePMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageLCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageNZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-942d6705cee9>\u001b[0m in \u001b[0;36mrun_GuidedLDA\u001b[0;34m(chats, anchorList, n_topics, n_words_per_topic, SEED_CONFIDENCE)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mseed_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED_CONFIDENCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Display and write to file the results of CorEx with no anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lda/guidedlda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, seed_topics, seed_confidence)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_confidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lda/guidedlda.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, seed_topics, seed_confidence)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mrands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_confidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# FIXME: using numpy.roll with a random shift might be faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lda/guidedlda.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X, seed_topics, seed_confidence)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# + W * self.beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_to_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lda/utils.py\u001b[0m in \u001b[0;36mmatrix_to_lists\u001b[0;34m(doc_word)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mWS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(a, repeats, axis)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \"\"\"\n\u001b[0;32m--> 479\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
          ]
        }
      ],
      "source": [
        "#P3\n",
        "\n",
        "\"\"\" File:  P3_semi_supervised_topic_modeling.py  \n",
        "\n",
        "    Description:  Loads a previously created pre-processed chat corpus, then performs\n",
        "    semi-supervised topic modeling utilizing CorEx and GuidedLDA.\n",
        "\n",
        "    INPUT FILES:\n",
        "    0) anchors.txt - anchor/seed words each on their own line\n",
        "    \n",
        "    Previously created preprocessed chat corpus from either:\n",
        "    1) wholeChatsFilePOS_N_ADJ_V.txt -- preprocessing keeping nouns, adjectives, and verbs\n",
        "    2) wholeChatsFilePOS_N_ADJ.txt -- preprocessing keeping nouns and adjectives\n",
        "    3) wholeChatsFile.txt -- NO POS preprocessing so all parts of speech\n",
        "    4) onlyQuestionsFile.txt -- Only initial question of chats\n",
        "\n",
        "    OUTPUT FILES:\n",
        "    1) \"raw_\" text (.txt) file listing topics with each word scored\n",
        "    2) \"LDA_\" text (.txt) file containing only the text for the\n",
        "       specified number of topics with the specified number of words per topic\n",
        "\n",
        "    Acknowledgements:\n",
        "\n",
        "    Here we are used the CorEx (Correlation Explanation) package available at GitHub:\n",
        "    https://github.com/gregversteeg/corex_topic\n",
        "\n",
        "    Here we are used the GuidedLDA package is available at GitHub:\n",
        "    https://github.com/vi3k6i5/GuidedLDA\n",
        "    NOTE:  We had difficulty installing GuidedLDA, but we were finally successful\n",
        "    by following the work-around posted at:\n",
        "    https://github.com/dex314/GuidedLDA_WorkAround\n",
        "\n",
        "\"\"\"\n",
        "import os.path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from corextopic import corextopic as ct\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from time import time\n",
        "\n",
        "import re, sys, random, math\n",
        "import numpy as np\n",
        "from lda import guidedlda as glda\n",
        "from lda import glda_datasets as gldad\n",
        "\n",
        "from collections import Counter\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# from P2_utility_functions import *\n",
        "# from P3_utility_functions import *\n",
        "\n",
        "def main():\n",
        "    print('Welcome to Phase 3 which runs the semi-supervised topic modeling techniques.',\n",
        "          '\\n\\nYou should have first run Phase 1 to pre-process your chat data.',\n",
        "          '\\nIt would generate cleaned chat files varying the parts of speech or question-only.',\n",
        "          '\\nFiles generated are: wholeChatsFile.txt, wholeChatsFilePOS_N_ADJ_V.txt,',\n",
        "          '\\nwholeChatsFilePOS_N_ADJ.txt, and onlyQuestionsFile.txt.\\n\\n')\n",
        "    print('\\n\\nYou could have also run Phase 2 to execute unsupervised topic modeling techniques.',\n",
        "          '\\nIt would generate files: possible_2_word_anchors.txt and possible_3_word_anchors.txt which',\n",
        "          '\\nyou might use to create a text-file (.txt) with anchors one per line.\\n')\n",
        "\n",
        "    prompt = \"\\nStep 1. Please input the pre-processed (.txt) file.\" + \\\n",
        "             '\\n(For example: \"wholeChatsFile.txt\"):'\n",
        "    fileName = getFileName(prompt)\n",
        "    chats = readChatCorpusFile(fileName)\n",
        "\n",
        "    prompt = \"\\nStep 2. Please input the anchors/seeds (.txt) file.\" + \\\n",
        "             '\\n(For example: \"anchors.txt\"):'\n",
        "    fileName = getFileName(prompt)\n",
        "    anchorList = readAnchorsFile(fileName)\n",
        "\n",
        "    modelDict = {'GuidedLDA':run_GuidedLDA,'CorEx':run_CorEx}\n",
        "\n",
        "    n_topics = getPositiveInteger('\\nStep 3. Please specify the number of topics. (suggested range 10-20)\\n')\n",
        "    n_words_per_topic = getPositiveInteger('\\nStep 4. Please specify the number of words per topics. (suggested range 5-10)\\n')\n",
        "\n",
        "    for model in modelDict:\n",
        "        print(\"=\"*35)\n",
        "        print(\"\\nPerforming\", model,\"topic modeling -- please wait it might take a couple minutes!\")\n",
        "        topicList = modelDict[model](chats, anchorList, n_topics, n_words_per_topic)\n",
        "        averagePMI, averageLCP, averageNZ = calculateTopicCoherenceMetrics(chats, topicList)\n",
        "        print(\"\\nResults for\",model,\" TC-PMI %3.3f, TC-LCP %3.3f, TC-NZ %3.3f:\" % (averagePMI, averageLCP, averageNZ))\n",
        "        for topic in topicList:\n",
        "            print(topic)\n",
        "        \n",
        "\n",
        "       \n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TestScripts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5ff56ed8d51e26a12c342d3b838f8895a05e9a18755e55247acd767e4e5d1178"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
